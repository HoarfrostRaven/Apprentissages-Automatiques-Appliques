{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 98233,
          "databundleVersionId": 11732442,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoarfrostRaven/Apprentissages-Automatiques-Appliques/blob/main/IMDB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB数据集的二分类情感分析\n",
        "\n",
        "大纲如下：\n",
        "- [Model](#Model-Part)\n",
        "- [DataLoader](#DataLoader-Part)\n",
        "- [Train_loop & Test_loop](#Train_loop-Part)\n",
        "- [Controller](#Controller-Part)\n",
        "- <span style='font-weight: bold; font-size: 1.2em; color:green;'>[Console](#Console-Part)</span>\n",
        "- [Evaluater](#Evaluater-Part)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T04:29:48.510935Z",
          "iopub.execute_input": "2025-04-09T04:29:48.511272Z",
          "iopub.status.idle": "2025-04-09T04:29:48.517657Z",
          "shell.execute_reply.started": "2025-04-09T04:29:48.511245Z",
          "shell.execute_reply": "2025-04-09T04:29:48.515852Z"
        },
        "id": "SdalfxErWjXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入需要使用的`Python package`"
      ],
      "metadata": {
        "id": "7bZ6XqR8WjXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Callable\n",
        "from pathlib import Path\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:14.968198Z",
          "iopub.execute_input": "2025-04-09T06:12:14.968394Z",
          "iopub.status.idle": "2025-04-09T06:12:18.717054Z",
          "shell.execute_reply.started": "2025-04-09T06:12:14.968375Z",
          "shell.execute_reply": "2025-04-09T06:12:18.716167Z"
        },
        "id": "vgL3AiQ8WjXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Model` Part"
      ],
      "metadata": {
        "id": "9hv3G2pAWjXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tranformer\n",
        "- `TransformerBlock`实现Transformer架构的核心编码器块，包含自注意力机制和前馈神经网络。该模块使用残差连接、层归一化和dropout来提高模型性能和稳定性。\n",
        "\n",
        "   `nn.MultiheadAttention`:对于每一个$\\text{head}_{i}$, 从`Input`出发，经过矩阵乘法得到$Q_{i},K_{i},V_{i}$，并对他们使用注意力机制，得到单个头的输出，再用$W^{O}$整合所有头的输出，得到我们最终的`Output`，其中$W^{Q}_{i}, W^{K}_{i}, W^{V}_{i}, W^{O}$是可训练的网络参数：\n",
        "   $$Q_{i}, K_{i}, V_{i} = W^{Q}_{i} \\times \\text{Input}, \\quad W^{K}_{i} \\times \\text{Input}, \\quad W^{V}_{i} \\times \\text{Input}$$  \n",
        "   $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_{1}, \\cdots, \\text{head}_{h})W^{O}$$\n",
        "   $$\\text{head}_{i} = \\text{Attention}(Q_{i}, K_{i}, V_{i})$$\n",
        "   $$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$$  \n",
        "   `forward`: 多头注意力的输出经过残差连接和层归一化：\n",
        "   $$X_{\\text{norm}} = \\text{LayerNorm}(\\text{Input} + \\text{MultiHead}(Q, K, V))$$  \n",
        "   然后，通过前馈神经网络（FFN）处理：\n",
        "   $$\\text{FFN}(X) = \\text{ReLU}(X \\cdot W_1 + b_1) \\cdot W_2 + b_2$$  \n",
        "   最后，再次应用残差连接和层归一化：\n",
        "   $$\\text{Output} = \\text{LayerNorm}(X_{\\text{norm}} + \\text{FFN}(X_{\\text{norm}}))$$\n",
        "\n",
        "- `TokenAndPositionEmbedding`:Token和位置嵌入模块, 将输入token索引转换为密集向量表示，并添加位置编码信息。位置编码使模型能够理解序列中token的相对位置。\n",
        "\n",
        "- `Transformer`: 基于Transformer的文本分类模型，此模型实现了为文本分类任务设计的简化版Transformer架构。"
      ],
      "metadata": {
        "id": "yz_MyDrAqEU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Block\n",
        "\n",
        "    参数:\n",
        "        embed_dim (int): 输入特征的维度\n",
        "        num_heads (int): 注意力头的数量\n",
        "        ff_dim (int): 前馈网络中的隐藏维度\n",
        "        rate (float, optional): Dropout率(默认为0.1)\n",
        "\n",
        "    输入:\n",
        "        inputs (torch.Tensor): 形状为[batch_size, sequence_length, embed_dim]的输入张量\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 与输入形状相同的转换后的特征\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        rate: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dense_1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.dense_2 = nn.Linear(ff_dim, embed_dim)\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(rate)\n",
        "        self.dropout_2 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        attn_output, _ = self.attention(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out1 = self.layer_norm_1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.dense_1(out1)\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = self.dense_2(ffn_output)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "\n",
        "        return self.layer_norm_2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "Mo7ZE2bR7RNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    参数:\n",
        "        max_length (int): 支持的最大序列长度\n",
        "        vocab_size (int): 词汇表大小\n",
        "        embed_dim (int): 嵌入向量的维度\n",
        "\n",
        "    输入:\n",
        "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 形状为[batch_size, sequence_length, embed_dim]的嵌入向量\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_length: int, vocab_size: int, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        ## 可学习的位置编码\n",
        "        # self.pos_emb = nn.Embedding(max_length, embed_dim)\n",
        "\n",
        "        ## 原论文中的位置编码\n",
        "        pe = torch.zeros(max_length, embed_dim).cuda()\n",
        "        positions = torch.arange(0, max_length, device=pe.device).unsqueeze(1)\n",
        "        div_term = (1 / (10000**(torch.arange(0, embed_dim, 2).float() / embed_dim))).cuda()\n",
        "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
        "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        self.register_buffer(\"positional_encoding\", pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## 可学习的位置编码\n",
        "        # positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        # return self.token_emb(x) + self.pos_emb(positions)\n",
        "\n",
        "        ## 原论文中的位置编码\n",
        "        maxlen = x.size(-1)\n",
        "        x = self.token_emb(x)\n",
        "        pos_enc = self.positional_encoding[:maxlen,:].unsqueeze(0)\n",
        "        return x + pos_enc"
      ],
      "metadata": {
        "id": "Njmw4e3y7UH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        embed_dim (int): token嵌入的维度大小\n",
        "        num_heads (int): 注意力头的数量\n",
        "        ff_dim (int): 前馈网络中的隐藏层大小\n",
        "        num_block(int): Transformer Block的数量\n",
        "        maxlen (int): 最大输入序列长度\n",
        "        vocab_size (int): 词汇表大小\n",
        "\n",
        "    输入:\n",
        "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 形状为[batch_size, 2]的浮点数组(两个类别的概率分布)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        num_blocks:int,\n",
        "        maxlen: int,\n",
        "        vocab_size: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dense1 = nn.Linear(embed_dim, 20)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.dense2 = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.dense1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = F.softmax(x, dim=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MlJmKrF37W5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n",
        "\n",
        "- `nn.LSTM`: 应用一个多层的长短期记忆网络（LSTM）到输入序列中。对于输入序列中的每一个元素，每一层执行以下计算：\n",
        "  $$\n",
        "  \\begin{aligned}\n",
        "   i_t &= \\sigma(W_{\\text{ii}}x_t + b_{\\text{ii}} + W_{\\text{hi}}h_{t-1} + b_{\\text{hi}}) \\\\\n",
        "   f_t &= \\sigma(W_{\\text{if}}x_t + b_{\\text{if}} + W_{\\text{hf}}h_{t-1} + b_{\\text{hf}}) \\\\\n",
        "   g_t &= \\tanh(W_{\\text{ig}}x_t + b_{\\text{ig}} + W_{\\text{hg}}h_{t-1} + b_{\\text{hg}}) \\\\\n",
        "   o_t &= \\sigma(W_{\\text{io}}x_t + b_{\\text{io}} + W_{\\text{ho}}h_{t-1} + b_{\\text{ho}}) \\\\\n",
        "   \\color{orange}{c_t} \\, &\\color{orange}{=f_t \\odot c_{t-1} + i_t \\odot g_t} \\\\\n",
        "   h_t &= o_t \\odot \\tanh(c_t)\n",
        "   \\end{aligned}\n",
        "   $$\n",
        "   其中：\n",
        "   - $h_t$ 表示时间步 $t$ 的隐藏状态；\n",
        "   - $c_t$ 表示时间步 $t$ 的神经元状态（cell state）；\n",
        "   - $x_t$ 是时间步 $t$ 的输入；\n",
        "   - $h_{t-1}$ 是前一个时间步的隐藏状态，或者是初始的隐藏状态（当 $t = 0$ 时）；\n",
        "   - $i_t, f_t, g_t, o_t$ 分别是输入门、遗忘门、候选状态、输出门；\n",
        "   - $\\sigma$ 是 sigmoid 激活函数；\n",
        "   - $\\odot$ 表示 Hadamard（元素乘）运算。\n",
        "\n",
        "- `LSTM`: 由一个`lstm`网络和一个全连接网络`FNN`组成\n",
        "\n",
        "   `forward`:\n",
        "   $$\n",
        "   \\begin{aligned}\n",
        "   \\text{output}, \\text{cell} = \\text{lstm}(\\text{input}) \\\\\n",
        "   \\text{probs} = \\text{Softmax}(\\text{FNN}())\n",
        "   \\end{aligned}\n",
        "   $$"
      ],
      "metadata": {
        "id": "vLqsEqJLp9bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size : int,\n",
        "                embedding_dim : int,\n",
        "                hidden_dim : int,\n",
        "                n_layers:int = 1,\n",
        "                dropout:float = 0.2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = n_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout if n_layers > 1 else 0\n",
        "                            )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 2) # 二分类问题\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        _, (hidden_t, _) = self.lstm(embedded)\n",
        "        logits = self.fc(self.dropout(hidden_t[-1, :, :]))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.718228Z",
          "iopub.execute_input": "2025-04-09T06:12:18.71871Z",
          "iopub.status.idle": "2025-04-09T06:12:18.738463Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.718677Z",
          "shell.execute_reply": "2025-04-09T06:12:18.737574Z"
        },
        "id": "mTgAiy1KWjXr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `DataLoader` Part\n",
        "\n",
        "- `IMDBDataset`：我们实现了`Pytorch`抽象类`Dataset`的一个子类，用于我们文本分类的任务。（所有`Dataset`子类必须要实现`__getitem__`方法，可选择实现`__len__`方法用于返回数据集的大小）\n",
        "\n",
        "`prepare_data_loader`: 准备训练和验证的数据加载器的函数。此函数加载数据集，将其分为训练集和验证集，并创建相应的数据加载器。数据加载过程使用`PyTorch`的`DataLoader`进行并行处理，以提高数据加载效率。工作流程：  \n",
        "- 从指定路径加载数据集\n",
        "- 根据给定比例将数据集分为训练集和验证集\n",
        "- 为训练集和验证集创建`Dataset`对象\n",
        "- 配置数据加载器，设置批量大小、工作进程数等\n",
        "\n",
        "返回一个包含训练和验证数据集的字典（{\"train\": `train_loader`, \"test\": `test_loader`}）"
      ],
      "metadata": {
        "id": "dfBnUMT1WjXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self._x = x\n",
        "        self._y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"encoded_indices\": torch.tensor(self._x[idx], dtype=torch.long),\n",
        "            \"label\": torch.tensor(self._y[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._x)"
      ],
      "metadata": {
        "id": "fs3x5y2R5kG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_loader(\n",
        "    path: str,\n",
        "    ratio: float = 0.2,\n",
        "    batch_size: int = 128,\n",
        "    seed: int = 12,\n",
        "    num_workers: int = 4,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        path (str): .npz格式的数据集文件路径\n",
        "        ratio (float, optional): 用于验证的数据比例(默认为0.2)\n",
        "        batch_size (int, optional): 训练批量大小(默认为128)\n",
        "        seed (int, optional): 用于随机打乱的种子(默认为12)\n",
        "        num_workers (int, optional): 数据加载的工作进程数(默认为4)\n",
        "\n",
        "    返回:\n",
        "        dict: 包含训练和测试数据加载器的字典\n",
        "    \"\"\"\n",
        "    train_data = np.load(path)\n",
        "\n",
        "    x_data = train_data[\"X\"]\n",
        "    y_data = train_data[\"y\"]\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x_data, y_data,\n",
        "        test_size=ratio,         # 20% 的数据作为验证集\n",
        "        random_state=seed,         # 固定随机种子，保证结果可复现\n",
        "        stratify=y_data          # 按标签比例划分，确保验证集和训练集标签分布一致\n",
        "    )\n",
        "\n",
        "    train_batch_size = batch_size\n",
        "    test_batch_size = train_batch_size\n",
        "\n",
        "    # 创建PyTorch数据集\n",
        "    train_dataset = IMDBDataset(x_train, y_train)\n",
        "    test_dataset = IMDBDataset(x_test, y_test)\n",
        "\n",
        "    # 创建数据加载器\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return {\"train\": train_loader, \"test\": test_loader}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.739837Z",
          "iopub.execute_input": "2025-04-09T06:12:18.740139Z",
          "iopub.status.idle": "2025-04-09T06:12:18.761351Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.74011Z",
          "shell.execute_reply": "2025-04-09T06:12:18.760545Z"
        },
        "id": "h_hsIuMfWjXs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Train_loop` Part\n",
        "这一部分是实现神经网络在一个训练周期中的两个关键步骤：\n",
        "- 在训练集上面计算损失，并依据损失更新参数\n",
        "- 在测试集上面计算度量(Metric)，跟踪当前参数下的泛化性能\n",
        "\n",
        "我们实现了`train_step`和`eval_step`两个函数分别执行单步(step)的训练步骤和测试步骤，`train_per_epoch`和`test_per_epoch`两个函数分别执行一个回合(epoch)的训练步骤和测试步骤。对于深度学习而言，一个`step`包含对一批(`batch`)数据的处理，一个`epoch`包含多个`step`，当把数据集的数据都训练过一遍，我们称为一个`epoch`，也就是说当数据集能被`batch`整除的时候，有:\n",
        "$$ \\text{batch\\_sizes} * \\text{num\\_steps} = \\text{total\\_data} $$\n",
        "\n",
        "`train_per_epoch` : 此函数处理一个完整`epoch`的训练过程:\n",
        "- 设置模型为训练模式\n",
        "- 遍历训练数据加载器中的所有批次\n",
        "- 对每个批次执行前向和后向传播并使用优化器更新模型参数\n",
        "- 使用进度条跟踪和显示训练指标\n",
        "\n",
        "`test_per_epoch` : 此函数处理一个完整`epoch`训练后的评估过程:\n",
        "- 设置模型为评估模式\n",
        "- 遍历测试数据加载器中的所有批次\n",
        "- 在不更新梯度的情况下计算前向传播\n",
        "- 计算评估指标（损失、准确率等），记录并显示评估结果\n"
      ],
      "metadata": {
        "id": "yisViD3qWjXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    model: nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    optimizer: optim.Optimizer,\n",
        "    batch: dict,\n",
        "    device: torch.device\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 要训练的模型\n",
        "        loss_fn (Callable): 损失函数\n",
        "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
        "        batch (dict): 包含训练数据的批次\n",
        "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
        "    返回:\n",
        "        float: 当前批次的损失值\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    batch_tokens = batch[\"encoded_indices\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "\n",
        "    loss = loss_fn(model(batch_tokens), labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "aLS7h3BP5u8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(model: nn.Module, metric_fn: Callable, batch: dict, device: torch.device):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 用于测试的模型\n",
        "        metric_fn (Callable): 测试集上的度量函数\n",
        "        batch (dict): 测试数据的批次\n",
        "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
        "\n",
        "    返回:\n",
        "        tuple: (loss, logits, labels) - 损失值、预测结果和真实标签\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_tokens = batch[\"encoded_indices\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        logits = model(batch_tokens)\n",
        "        metric = metric_fn(logits, labels)\n",
        "\n",
        "        return metric.item(), logits, labels"
      ],
      "metadata": {
        "id": "e0T_k2fu5w-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_per_epoch(\n",
        "    model: nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    optimizer: optim.Optimizer,\n",
        "    batch_size: int,\n",
        "    train_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 训练模型\n",
        "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
        "        train_loader (DataLoader): 包含训练数据的DataLoader\n",
        "        device (torch.device): 用于计算的设备\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    length = len(train_loader.dataset)\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        loss = train_step(model, loss_fn, optimizer, batch, device)\n",
        "        if batch_idx % 20 == 0:\n",
        "            current = batch_idx * batch_size + len(batch[\"encoded_indices\"])\n",
        "            print(f\" Loss: {loss:>6.4f}, {current:>5d}/{length:>5d}\")"
      ],
      "metadata": {
        "id": "K44Ijl6w5yun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_per_epoch(\n",
        "    model: nn.Module,\n",
        "    metric_fn: Callable,\n",
        "    test_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 测试模型\n",
        "        test_loader (DataLoader): 包含测试数据的DataLoader\n",
        "        device (torch.device): 用于计算的设备\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    num_batches = len(test_loader)\n",
        "    num_data = len(test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            loss, logits, labels = eval_step(model, metric_fn, batch, device)\n",
        "            total_loss += loss\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = correct / num_data\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.763181Z",
          "iopub.execute_input": "2025-04-09T06:12:18.763371Z",
          "iopub.status.idle": "2025-04-09T06:12:18.782085Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.763354Z",
          "shell.execute_reply": "2025-04-09T06:12:18.781257Z"
        },
        "id": "liqj92foWjXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Controller` Part\n",
        "这一部分是我们这份代码的控制中枢，我们实例化上文已经写好的类，调用上文已经写好的各个函数，组合起来实现我们的IMDB数据集的情感分析任务。在这一部分，我们要指定需要设定的绝大部分超参数，开展训练并得到我们训练好的模型。"
      ],
      "metadata": {
        "id": "Sv6yDEtlWjXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def controller(seed: int,\n",
        "               model_type: str,\n",
        "               embed_dim: int,\n",
        "               num_heads: int,\n",
        "               ff_dim: int,\n",
        "               num_blocks:int,\n",
        "               lstm_embed: int,\n",
        "               hidden_dim: int,\n",
        "               ratio: float,\n",
        "               batch_size: int,\n",
        "               epochs: int,\n",
        "               learning_rate: float):\n",
        "    torch.manual_seed(seed)\n",
        "    data_path = Path(\"processed_imdb_train_data.npz\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if model_type == \"Transformer\":\n",
        "        model = Transformer(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            num_blocks=num_blocks,\n",
        "            maxlen=200,\n",
        "            vocab_size=20000,\n",
        "        ).to(device)\n",
        "    elif model_type == \"LSTM\":\n",
        "        model = LSTM(\n",
        "            vocab_size=20000,\n",
        "            embedding_dim=lstm_embed,\n",
        "            hidden_dim=hidden_dim,\n",
        "        ).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    metric_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    loader_dict = prepare_data_loader(data_path, ratio, batch_size, seed)\n",
        "    train_loader = loader_dict[\"train\"]\n",
        "    test_loader = loader_dict[\"test\"]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} \\n--------------------------------\")\n",
        "        train_per_epoch(model, loss_fn, optimizer, batch_size, train_loader, device)\n",
        "        # 固定学习率 vs 动态学习率\n",
        "        # scheduler.step() # 每个epoch结束时更新学习率\n",
        "        test_per_epoch(model, metric_fn, test_loader, device)\n",
        "    print(\"Done!\")\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.783178Z",
          "iopub.execute_input": "2025-04-09T06:12:18.783475Z",
          "iopub.status.idle": "2025-04-09T06:12:18.802641Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.783445Z",
          "shell.execute_reply": "2025-04-09T06:12:18.801852Z"
        },
        "id": "aicM8ClzWjXu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Console` Part\n",
        "这个板块是我们的总控制台，在这里我们设定所有的超参数，调用我们前面写好的所有类和函数，训练模型并返回我们的模型"
      ],
      "metadata": {
        "id": "0IMfRaVpWjXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 12 # 设置我们的全局随机种子\n",
        "\n",
        "model_type = \"LSTM\" # 指定要选择的模型[\"Transformer\", \"LSTM\"]\n",
        "# Transformer模型相关的超参数, 如果是LSTM则不需要关注\n",
        "# embed_dim 必须能整除 num_heads，否则无法分头\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "num_blocks = 2\n",
        "\n",
        "# LSTM模型相关的超参数\n",
        "lstm_embed = 32\n",
        "hidden_dim = 32\n",
        "\n",
        "ratio = 0.2 #测试集占数据集的比例\n",
        "batch_size = 128 # batch的大小\n",
        "epochs = 50 # 训练回合数\n",
        "learning_rate = 5e-4 #初始学习率的大小\n",
        "\n",
        "\n",
        "model = controller(seed,\n",
        "                   model_type,\n",
        "                   embed_dim,\n",
        "                   num_heads,\n",
        "                   ff_dim,\n",
        "                   num_blocks,\n",
        "                   lstm_embed,\n",
        "                   hidden_dim,\n",
        "                   ratio,\n",
        "                   batch_size,\n",
        "                   epochs,\n",
        "                   learning_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:14:08.689548Z",
          "iopub.execute_input": "2025-04-09T06:14:08.689902Z",
          "iopub.status.idle": "2025-04-09T06:15:00.407758Z",
          "shell.execute_reply.started": "2025-04-09T06:14:08.689872Z",
          "shell.execute_reply": "2025-04-09T06:15:00.406863Z"
        },
        "id": "f6Syk95aWjXv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "a713913a-1693-42b6-d2ef-77dcfe478879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loss: 0.6944,   128/20000\n",
            " Loss: 0.6937,  2688/20000\n",
            " Loss: 0.6886,  5248/20000\n",
            " Loss: 0.6936,  7808/20000\n",
            " Loss: 0.6941, 10368/20000\n",
            " Loss: 0.6855, 12928/20000\n",
            " Loss: 0.6913, 15488/20000\n",
            " Loss: 0.6916, 18048/20000\n",
            "Test Error: \n",
            " Accuracy: 54.0%, Avg loss: 0.688675 \n",
            "\n",
            "Epoch 2 \n",
            "--------------------------------\n",
            " Loss: 0.6828,   128/20000\n",
            " Loss: 0.6849,  2688/20000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1a172ef1380c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model = controller(seed,\n\u001b[0m\u001b[1;32m     21\u001b[0m                    \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                    \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-66c5b3d063c3>\u001b[0m in \u001b[0;36mcontroller\u001b[0;34m(seed, model_type, embed_dim, num_heads, ff_dim, num_blocks, lstm_embed, hidden_dim, ratio, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1} \\n--------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrain_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# scheduler.step() # 每个epoch结束时更新学习率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtest_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-958954eb06e0>\u001b[0m in \u001b[0;36mtrain_per_epoch\u001b[0;34m(model, loss_fn, optimizer, batch_size, train_loader, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoded_indices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-958954eb06e0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, loss_fn, optimizer, batch, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Evaluater` Part\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>这部分代码同学们不用修改调参，不会影响我们训练模型的性能。\n",
        "</div>\n",
        "\n",
        "这一部分我们调用已经训练好的模型，去预测我们`kaggle`给出的不带标签(`label`)的测试数据集，并将我们预测的结果写成一个`submission.csv`文件，放在指定的`kaggle`云文件夹下，便于自动评估我们提交模型的表现。"
      ],
      "metadata": {
        "id": "fB_K2-t_WjXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluater(model: nn.Module):\n",
        "    model.eval()\n",
        "    test_data_path = Path(\"processed_imdb_test_data.npz\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_data = np.load(test_data_path)\n",
        "        test_tokens = test_data[\"X\"]  # shape: (num_samples, 200)\n",
        "        test_labels = test_data[\"y\"]  # shape: (num_samples,)\n",
        "\n",
        "        all_preds = []\n",
        "\n",
        "        batch_size = 500\n",
        "        num_batches = int(np.ceil(len(test_tokens) / batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = min((i + 1) * batch_size, len(test_tokens))\n",
        "            batch_test_tokens = torch.tensor(test_tokens[start:end], dtype=torch.long).cuda()\n",
        "\n",
        "            logits = model(batch_test_tokens)\n",
        "            _, batch_predicted_labels = torch.max(logits, dim=-1)\n",
        "            all_preds.append(batch_predicted_labels.cpu().numpy())\n",
        "\n",
        "        # 拼接所有预测结果\n",
        "        predicted_labels = np.concatenate(all_preds)\n",
        "        # 计算准确率\n",
        "        accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "        print(f\"测试集准确率：{accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "vyRrg98VROEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluater(model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:13:13.746107Z",
          "iopub.execute_input": "2025-04-09T06:13:13.746773Z",
          "iopub.status.idle": "2025-04-09T06:13:14.160425Z",
          "shell.execute_reply.started": "2025-04-09T06:13:13.746744Z",
          "shell.execute_reply": "2025-04-09T06:13:14.159758Z"
        },
        "id": "BFJ84SKyWjXw"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}