{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 98233,
          "databundleVersionId": 11732442,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoarfrostRaven/Apprentissages-Automatiques-Appliques/blob/main/IMDB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "LRqjn21SWjXh"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "sjtu_ai_0410_path = kagglehub.competition_download('sjtu-ai-0410')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "dkfPJQTwWjXl"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB数据集的二分类情感分析\n",
        "\n",
        "大纲如下：\n",
        "- [Model](#Model-Part)\n",
        "- [DataLoader](#DataLoader-Part)\n",
        "- [Train_loop & Test_loop](#Train_loop-Part)\n",
        "- [Controller](#Controller-Part)\n",
        "- <span style='font-weight: bold; font-size: 1.2em; color:green;'>[Console](#Console-Part)</span>\n",
        "- [Evaluater](#Evaluater-Part)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T04:29:48.510935Z",
          "iopub.execute_input": "2025-04-09T04:29:48.511272Z",
          "iopub.status.idle": "2025-04-09T04:29:48.517657Z",
          "shell.execute_reply.started": "2025-04-09T04:29:48.511245Z",
          "shell.execute_reply": "2025-04-09T04:29:48.515852Z"
        },
        "id": "SdalfxErWjXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入需要使用的`Python package`"
      ],
      "metadata": {
        "id": "7bZ6XqR8WjXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Callable\n",
        "from pathlib import Path\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:14.968198Z",
          "iopub.execute_input": "2025-04-09T06:12:14.968394Z",
          "iopub.status.idle": "2025-04-09T06:12:18.717054Z",
          "shell.execute_reply.started": "2025-04-09T06:12:14.968375Z",
          "shell.execute_reply": "2025-04-09T06:12:18.716167Z"
        },
        "id": "vgL3AiQ8WjXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Model` Part\n",
        "\n",
        "### Tranformer\n",
        "- `TransformerBlock`实现Transformer架构的核心编码器块，包含自注意力机制和前馈神经网络。该模块使用残差连接、层归一化和dropout来提高模型性能和稳定性。\n",
        "\n",
        "   `nn.MultiheadAttention`:对于每一个$\\text{head}_{i}$, 从`Input`出发，经过矩阵乘法得到$Q_{i},K_{i},V_{i}$，并对他们使用注意力机制，得到单个头的输出，再用$W^{O}$整合所有头的输出，得到我们最终的`Output`，其中$W^{Q}_{i}, W^{K}_{i}, W^{V}_{i}, W^{O}$是可训练的网络参数：\n",
        "   $$Q_{i}, K_{i}, V_{i} = W^{Q}_{i} \\times \\text{Input}, \\quad W^{K}_{i} \\times \\text{Input}, \\quad W^{V}_{i} \\times \\text{Input}$$  \n",
        "   $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_{1}, \\cdots, \\text{head}_{h})W^{O}$$\n",
        "   $$\\text{head}_{i} = \\text{Attention}(Q_{i}, K_{i}, V_{i})$$\n",
        "   $$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$$  \n",
        "   `forward`: 多头注意力的输出经过残差连接和层归一化：\n",
        "   $$X_{\\text{norm}} = \\text{LayerNorm}(\\text{Input} + \\text{MultiHead}(Q, K, V))$$  \n",
        "   然后，通过前馈神经网络（FFN）处理：\n",
        "   $$\\text{FFN}(X) = \\text{ReLU}(X \\cdot W_1 + b_1) \\cdot W_2 + b_2$$  \n",
        "   最后，再次应用残差连接和层归一化：\n",
        "   $$\\text{Output} = \\text{LayerNorm}(X_{\\text{norm}} + \\text{FFN}(X_{\\text{norm}}))$$\n",
        "\n",
        "- `TokenAndPositionEmbedding`:Token和位置嵌入模块, 将输入token索引转换为密集向量表示，并添加位置编码信息。位置编码使模型能够理解序列中token的相对位置。\n",
        "\n",
        "- `Transformer`: 基于Transformer的文本分类模型，此模型实现了为文本分类任务设计的简化版Transformer架构。\n",
        "\n",
        "### LSTM\n",
        "\n",
        "- `nn.LSTM`: 应用一个多层的长短期记忆网络（LSTM）到输入序列中。对于输入序列中的每一个元素，每一层执行以下计算：\n",
        "  $$\n",
        "  \\begin{aligned}\n",
        "   i_t &= \\sigma(W_{\\text{ii}}x_t + b_{\\text{ii}} + W_{\\text{hi}}h_{t-1} + b_{\\text{hi}}) \\\\\n",
        "   f_t &= \\sigma(W_{\\text{if}}x_t + b_{\\text{if}} + W_{\\text{hf}}h_{t-1} + b_{\\text{hf}}) \\\\\n",
        "   g_t &= \\tanh(W_{\\text{ig}}x_t + b_{\\text{ig}} + W_{\\text{hg}}h_{t-1} + b_{\\text{hg}}) \\\\\n",
        "   o_t &= \\sigma(W_{\\text{io}}x_t + b_{\\text{io}} + W_{\\text{ho}}h_{t-1} + b_{\\text{ho}}) \\\\\n",
        "   \\color{orange}{c_t} \\, &\\color{orange}{=f_t \\odot c_{t-1} + i_t \\odot g_t} \\\\\n",
        "   h_t &= o_t \\odot \\tanh(c_t)\n",
        "   \\end{aligned}\n",
        "   $$\n",
        "   其中：\n",
        "   - $h_t$ 表示时间步 $t$ 的隐藏状态；\n",
        "   - $c_t$ 表示时间步 $t$ 的神经元状态（cell state）；\n",
        "   - $x_t$ 是时间步 $t$ 的输入；\n",
        "   - $h_{t-1}$ 是前一个时间步的隐藏状态，或者是初始的隐藏状态（当 $t = 0$ 时）；\n",
        "   - $i_t, f_t, g_t, o_t$ 分别是输入门、遗忘门、候选状态、输出门；\n",
        "   - $\\sigma$ 是 sigmoid 激活函数；\n",
        "   - $\\odot$ 表示 Hadamard（元素乘）运算。\n",
        "\n",
        "- `LSTM`: 由一个`lstm`网络和一个全连接网络`FNN`组成\n",
        "\n",
        "   `forward`:\n",
        "   $$\n",
        "   \\begin{aligned}\n",
        "   \\text{output}, \\textcolor{orange}{hidden},\\text{cell} = \\text{lstm}(\\text{input}) \\\\\n",
        "   \\text{probs} = \\text{Softmax}(\\text{FNN}(\\textcolor{orange}{hidden}))\n",
        "   \\end{aligned}\n",
        "   $$"
      ],
      "metadata": {
        "id": "9hv3G2pAWjXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Block\n",
        "\n",
        "    参数:\n",
        "        embed_dim (int): 输入特征的维度\n",
        "        num_heads (int): 注意力头的数量\n",
        "        ff_dim (int): 前馈网络中的隐藏维度\n",
        "        rate (float, optional): Dropout率(默认为0.1)\n",
        "\n",
        "    输入:\n",
        "        inputs (torch.Tensor): 形状为[batch_size, sequence_length, embed_dim]的输入张量\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 与输入形状相同的转换后的特征\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        rate: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dense_1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.dense_2 = nn.Linear(ff_dim, embed_dim)\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(rate)\n",
        "        self.dropout_2 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        attn_output, _ = self.attention(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out1 = self.layer_norm_1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.dense_1(out1)\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = self.dense_2(ffn_output)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "\n",
        "        return self.layer_norm_2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    参数:\n",
        "        max_length (int): 支持的最大序列长度\n",
        "        vocab_size (int): 词汇表大小\n",
        "        embed_dim (int): 嵌入向量的维度\n",
        "\n",
        "    输入:\n",
        "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 形状为[batch_size, sequence_length, embed_dim]的嵌入向量\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_length: int, vocab_size: int, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        pe = torch.zeros(max_length, embed_dim).cuda()\n",
        "        positions = torch.arange(0, max_length, device=pe.device).unsqueeze(1)\n",
        "        # div_term = (torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))).cuda()\n",
        "        div_term = (1 / (10000**(torch.arange(0, embed_dim, 2).float() / embed_dim))).cuda()\n",
        "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
        "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        self.register_buffer(\"positional_encoding\", pe)\n",
        "\n",
        "        # self.pos_emb = nn.Embedding(max_length, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        maxlen = x.size(-1)\n",
        "        # positions = self.pos_emb(positions)\n",
        "\n",
        "        x = self.token_emb(x)\n",
        "        pos_enc = self.positional_encoding[:maxlen,:].unsqueeze(0)\n",
        "        return x + pos_enc\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        embed_dim (int): token嵌入的维度大小\n",
        "        num_heads (int): 注意力头的数量\n",
        "        ff_dim (int): 前馈网络中的隐藏层大小\n",
        "        num_block(int): Transformer Block的数量\n",
        "        maxlen (int): 最大输入序列长度\n",
        "        vocab_size (int): 词汇表大小\n",
        "\n",
        "    输入:\n",
        "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
        "\n",
        "    输出:\n",
        "        torch.Tensor: 形状为[batch_size, 2]的浮点数组(两个类别的概率分布)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        num_blocks:int,\n",
        "        maxlen: int,\n",
        "        vocab_size: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dense1 = nn.Linear(embed_dim, 20)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.dense2 = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.dense1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = F.softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                vocab_size : int,\n",
        "                embedding_dim : int,\n",
        "                hidden_dim : int,\n",
        "                n_layers:int = 1,\n",
        "                dropout:float = 0.2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = n_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout if n_layers > 1 else 0\n",
        "                            )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 2) # 二分类问题\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        _, (hidden_t, _) = self.lstm(embedded)\n",
        "        logits = self.fc(self.dropout(hidden_t[-1, :, :]))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.718228Z",
          "iopub.execute_input": "2025-04-09T06:12:18.71871Z",
          "iopub.status.idle": "2025-04-09T06:12:18.738463Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.718677Z",
          "shell.execute_reply": "2025-04-09T06:12:18.737574Z"
        },
        "id": "mTgAiy1KWjXr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `DataLoader` Part\n",
        "\n",
        "- `IMDBDataset`：我们实现了`Pytorch`抽象类`Dataset`的一个子类，用于我们文本分类的任务。（所有`Dataset`子类必须要实现`__getitem__`方法，可选择实现`__len__`方法用于返回数据集的大小）\n",
        "\n",
        "`prepare_data_loader`: 准备训练和测试的数据加载器的函数。此函数加载数据集，将其分为训练集和测试集，并创建相应的数据加载器。数据加载过程使用`PyTorch`的`DataLoader`进行并行处理，以提高数据加载效率。工作流程：  \n",
        "- 从指定路径加载数据集\n",
        "- 根据给定比例将数据集分为训练集和测试集\n",
        "- 为训练集和测试集创建`Dataset`对象\n",
        "- 配置数据加载器，设置批量大小、工作进程数等\n",
        "\n",
        "返回一个包含训练和测试数据集的字典（{\"train\": `train_loader`, \"test\": `test_loader`}）"
      ],
      "metadata": {
        "id": "dfBnUMT1WjXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self._x = x\n",
        "        self._y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"encoded_indices\": torch.tensor(self._x[idx], dtype=torch.long),\n",
        "            \"label\": torch.tensor(self._y[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._x)\n",
        "\n",
        "\n",
        "def prepare_data_loader(\n",
        "    path: str,\n",
        "    ratio: float,\n",
        "    batch_size: int,\n",
        "    num_workers: int = 4,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        path (str): .npz格式的数据集文件路径\n",
        "        ratio (float, optional): 用于训练的数据比例(默认为0.8)\n",
        "        seed (int, optional): 用于随机打乱的种子(默认为12)\n",
        "        batch_size (int, optional): 训练批量大小(默认为128)\n",
        "        num_workers (int, optional): 数据加载的工作进程数(默认为4)\n",
        "\n",
        "    返回:\n",
        "        dict: 包含训练和测试数据加载器的字典\n",
        "    \"\"\"\n",
        "    train_data = np.load(path)\n",
        "\n",
        "    x_data = train_data[\"x_train\"]\n",
        "    y_data = train_data[\"y_train\"]\n",
        "\n",
        "    num_samples = len(x_data)\n",
        "    split_idx = int(num_samples * ratio)\n",
        "    x_train = x_data[:split_idx]\n",
        "    y_train = y_data[:split_idx]\n",
        "    x_test = x_data[split_idx:]\n",
        "    y_test = y_data[split_idx:]\n",
        "\n",
        "    train_batch_size = batch_size\n",
        "    test_batch_size = train_batch_size\n",
        "\n",
        "    # 创建PyTorch数据集\n",
        "    train_dataset = IMDBDataset(x_train, y_train)\n",
        "    test_dataset = IMDBDataset(x_test, y_test)\n",
        "\n",
        "    # 创建数据加载器\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return {\"train\": train_loader, \"test\": test_loader}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.739837Z",
          "iopub.execute_input": "2025-04-09T06:12:18.740139Z",
          "iopub.status.idle": "2025-04-09T06:12:18.761351Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.74011Z",
          "shell.execute_reply": "2025-04-09T06:12:18.760545Z"
        },
        "id": "h_hsIuMfWjXs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Train_loop` Part\n",
        "这一部分是实现神经网络在一个训练周期中的两个关键步骤：\n",
        "- 在训练集上面计算损失，并依据损失更新参数\n",
        "- 在测试集上面计算度量(Metric)，跟踪当前参数下的泛化性能\n",
        "\n",
        "我们实现了`train_step`和`eval_step`两个函数分别执行单步(step)的训练步骤和测试步骤，`train_per_epoch`和`test_per_epoch`两个函数分别执行一个回合(epoch)的训练步骤和测试步骤。对于深度学习而言，一个`step`包含对一批(`batch`)数据的处理，一个`epoch`包含多个`step`，当把数据集的数据都训练过一遍，我们称为一个`epoch`，也就是说当数据集能被`batch`整除的时候，有:\n",
        "$$ \\text{batch\\_sizes} * \\text{num\\_steps} = \\text{total\\_data} $$\n",
        "\n",
        "`train_per_epoch` : 此函数处理一个完整`epoch`的训练过程:\n",
        "- 设置模型为训练模式\n",
        "- 遍历训练数据加载器中的所有批次\n",
        "- 对每个批次执行前向和后向传播并使用优化器更新模型参数\n",
        "- 使用进度条跟踪和显示训练指标\n",
        "\n",
        "`test_per_epoch` : 此函数处理一个完整`epoch`训练后的评估过程:\n",
        "- 设置模型为评估模式\n",
        "- 遍历测试数据加载器中的所有批次\n",
        "- 在不更新梯度的情况下计算前向传播\n",
        "- 计算评估指标（损失、准确率等），记录并显示评估结果\n"
      ],
      "metadata": {
        "id": "yisViD3qWjXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    model: nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    optimizer: optim.Optimizer,\n",
        "    batch: dict,\n",
        "    device: torch.device\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 要训练的模型\n",
        "        loss_fn (Callable): 损失函数\n",
        "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
        "        batch (dict): 包含训练数据的批次\n",
        "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
        "    返回:\n",
        "        float: 当前批次的损失值\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    batch_tokens = batch[\"encoded_indices\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "\n",
        "    loss = loss_fn(model(batch_tokens), labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def eval_step(model: nn.Module, metric_fn: Callable, batch: dict, device: torch.device):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 用于测试的模型\n",
        "        metric_fn (Callable): 测试集上的度量函数\n",
        "        batch (dict): 测试数据的批次\n",
        "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
        "\n",
        "    返回:\n",
        "        tuple: (loss, logits, labels) - 损失值、预测结果和真实标签\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_tokens = batch[\"encoded_indices\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        logits = model(batch_tokens)\n",
        "        metric = metric_fn(logits, labels)\n",
        "\n",
        "        return metric.item(), logits, labels\n",
        "\n",
        "\n",
        "def train_per_epoch(\n",
        "    model: nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    optimizer: optim.Optimizer,\n",
        "    batch_size: int,\n",
        "    train_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 训练模型\n",
        "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
        "        train_loader (DataLoader): 包含训练数据的DataLoader\n",
        "        device (torch.device): 用于计算的设备\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    length = len(train_loader.dataset)\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        loss = train_step(model, loss_fn, optimizer, batch, device)\n",
        "        if batch_idx % 20 == 0:\n",
        "            current = batch_idx * batch_size + len(batch[\"encoded_indices\"])\n",
        "            print(f\" Loss: {loss:>6.4f}, {current:>5d}/{length:>5d}\")\n",
        "\n",
        "\n",
        "def test_per_epoch(\n",
        "    model: nn.Module,\n",
        "    metric_fn: Callable,\n",
        "    test_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    参数:\n",
        "        model (nn.Module): 测试模型\n",
        "        test_loader (DataLoader): 包含测试数据的DataLoader\n",
        "        device (torch.device): 用于计算的设备\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    num_batches = len(test_loader)\n",
        "    num_data = len(test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            loss, logits, labels = eval_step(model, metric_fn, batch, device)\n",
        "            total_loss += loss\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = correct / num_data\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.763181Z",
          "iopub.execute_input": "2025-04-09T06:12:18.763371Z",
          "iopub.status.idle": "2025-04-09T06:12:18.782085Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.763354Z",
          "shell.execute_reply": "2025-04-09T06:12:18.781257Z"
        },
        "id": "liqj92foWjXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Controller` Part\n",
        "这一部分是我们这份代码的控制中枢，我们实例化上文已经写好的类，调用上文已经写好的各个函数，组合起来实现我们的IMDB数据集的情感分析任务。在这一部分，我们要指定需要设定的绝大部分超参数，开展训练并得到我们训练好的模型。"
      ],
      "metadata": {
        "id": "Sv6yDEtlWjXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def controller(seed: int,\n",
        "               model_type: str,\n",
        "               embed_dim: int,\n",
        "               num_heads: int,\n",
        "               ff_dim: int,\n",
        "               num_blocks:int,\n",
        "               lstm_embed: int,\n",
        "               hidden_dim: int,\n",
        "               ratio: float,\n",
        "               batch_size: int,\n",
        "               epochs: int,\n",
        "               learning_rate: float):\n",
        "    torch.manual_seed(seed)\n",
        "    data_path = Path(\"/kaggle/input/sjtu-ai-0410/processed_imdb_train_data.npz\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if model_type == \"Transformer\":\n",
        "        model = Transformer(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            num_blocks=num_blocks,\n",
        "            maxlen=200,\n",
        "            vocab_size=20000,\n",
        "        ).to(device)\n",
        "    elif model_type == \"LSTM\":\n",
        "        model = LSTM(\n",
        "            vocab_size=20000,\n",
        "            embedding_dim=lstm_embed,\n",
        "            hidden_dim=hidden_dim,\n",
        "        ).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    metric_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    loader_dict = prepare_data_loader(data_path, ratio, batch_size)\n",
        "    train_loader = loader_dict[\"train\"]\n",
        "    test_loader = loader_dict[\"test\"]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} \\n--------------------------------\")\n",
        "        train_per_epoch(model, loss_fn, optimizer, batch_size, train_loader, device)\n",
        "        # scheduler.step() # 每个epoch结束时更新学习率\n",
        "        test_per_epoch(model, metric_fn, test_loader, device)\n",
        "    print(\"Done!\")\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:12:18.783178Z",
          "iopub.execute_input": "2025-04-09T06:12:18.783475Z",
          "iopub.status.idle": "2025-04-09T06:12:18.802641Z",
          "shell.execute_reply.started": "2025-04-09T06:12:18.783445Z",
          "shell.execute_reply": "2025-04-09T06:12:18.801852Z"
        },
        "id": "aicM8ClzWjXu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Console` Part\n",
        "这个板块是我们的总控制台，在这里我们设定所有的超参数，调用我们前面写好的所有类和函数，训练模型并返回我们的模型"
      ],
      "metadata": {
        "id": "0IMfRaVpWjXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 12 # 设置我们的全局随机种子\n",
        "\n",
        "model_type = \"LSTM\" # 指定要选择的模型[\"Transformer\", \"LSTM\"]\n",
        "# Transformer模型相关的超参数, 如果是LSTM则不需要关注\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "num_blocks = 2\n",
        "\n",
        "# LSTM模型相关的超参数\n",
        "lstm_embed = 32\n",
        "hidden_dim = 32\n",
        "\n",
        "ratio = 0.8 #训练集占数据集的比例\n",
        "batch_size = 128 # batch的大小\n",
        "epochs = 50 # 训练回合数\n",
        "learning_rate = 5e-4 #初始学习率的大小\n",
        "\n",
        "\n",
        "model = controller(seed,\n",
        "                   model_type,\n",
        "                   embed_dim,\n",
        "                   num_heads,\n",
        "                   ff_dim,\n",
        "                   num_blocks,\n",
        "                   lstm_embed,\n",
        "                   hidden_dim,\n",
        "                   ratio,\n",
        "                   batch_size,\n",
        "                   epochs,\n",
        "                   learning_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:14:08.689548Z",
          "iopub.execute_input": "2025-04-09T06:14:08.689902Z",
          "iopub.status.idle": "2025-04-09T06:15:00.407758Z",
          "shell.execute_reply.started": "2025-04-09T06:14:08.689872Z",
          "shell.execute_reply": "2025-04-09T06:15:00.406863Z"
        },
        "id": "f6Syk95aWjXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Evaluater` Part\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>这部分代码同学们不用修改调参，不会影响我们训练模型的性能。\n",
        "</div>\n",
        "\n",
        "这一部分我们调用已经训练好的模型，去预测我们`kaggle`给出的不带标签(`label`)的测试数据集，并将我们预测的结果写成一个`submission.csv`文件，放在指定的`kaggle`云文件夹下，便于自动评估我们提交模型的表现。"
      ],
      "metadata": {
        "id": "fB_K2-t_WjXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluater(model: nn.Module):\n",
        "    model.eval()\n",
        "    test_data_path = Path(\"/kaggle/input/sjtu-ai-0410/processed_imdb_test_data.npz\")\n",
        "    submission_path = Path(\"/kaggle/working/submission.csv\")\n",
        "    with torch.no_grad():\n",
        "        test_data = np.load(test_data_path)\n",
        "        test_ids = test_data[\"ID\"]\n",
        "        test_tokens = test_data[\"x_test\"]\n",
        "        for i in range(5):\n",
        "            batch_test_tokens = torch.tensor(test_tokens[5000*i: 5000*(i+1), :], dtype=torch.long).cuda()\n",
        "            probs = model(batch_test_tokens)\n",
        "            _, batch_predicted_labels = torch.max(probs, dim=-1)\n",
        "            if i == 0:\n",
        "                predicted_labels = batch_predicted_labels.cpu().numpy()\n",
        "            else:\n",
        "                predicted_labels = np.hstack((predicted_labels, batch_predicted_labels.cpu().numpy()))\n",
        "\n",
        "    predicted_labels = pd.DataFrame(predicted_labels, index=test_ids, columns=[\"label\"])\n",
        "    predicted_labels.to_csv(submission_path, index=True, index_label=\"ID\")\n",
        "\n",
        "evaluater(model)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T06:13:13.746107Z",
          "iopub.execute_input": "2025-04-09T06:13:13.746773Z",
          "iopub.status.idle": "2025-04-09T06:13:14.160425Z",
          "shell.execute_reply.started": "2025-04-09T06:13:13.746744Z",
          "shell.execute_reply": "2025-04-09T06:13:14.159758Z"
        },
        "id": "BFJ84SKyWjXw"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}